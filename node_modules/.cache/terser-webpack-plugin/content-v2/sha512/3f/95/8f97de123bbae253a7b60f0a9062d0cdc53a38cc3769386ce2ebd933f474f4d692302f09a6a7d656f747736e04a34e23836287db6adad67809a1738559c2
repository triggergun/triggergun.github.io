{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[196],{1345:function(t,n,a){\"use strict\";a.r(n);var s=a(14),e=Object(s.a)({},(function(){var t=this,n=t._self._c;return n(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":t.$parent.slotKey}},[n(\"h1\",{attrs:{id:\"python爬虫必知\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#python爬虫必知\"}},[t._v(\"#\")]),t._v(\" python爬虫必知\")]),t._v(\" \"),n(\"h2\",{attrs:{id:\"cookie\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#cookie\"}},[t._v(\"#\")]),t._v(\" cookie\")]),t._v(\" \"),n(\"div\",{staticClass:\"custom-block tip\"},[n(\"p\",{staticClass:\"custom-block-title\"},[t._v(\"cookie是什么呢？\")]),t._v(\" \"),n(\"p\",[t._v(\"http协议，无状态\\n作用：网站登入时候的问题，用来记录用户身份的。\")])]),t._v(\" \"),n(\"p\",[t._v(\"模拟登入===》【人人网 】\\n抓包获取cookie，发送request\")]),t._v(\" \"),n(\"h2\",{attrs:{id:\"正则表达式解析\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#正则表达式解析\"}},[t._v(\"#\")]),t._v(\" 正则表达式解析\")]),t._v(\" \"),n(\"p\",[t._v(\"为什么引入正则表达式？\")]),t._v(\" \"),n(\"p\",[t._v(\"答：用来匹配一类具有相同规则的”字符串“。\\n规则：\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"_1-单字符\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_1-单字符\"}},[t._v(\"#\")]),t._v(\" 1)单字符：\")]),t._v(\" \"),n(\"table\",[n(\"thead\",[n(\"tr\",[n(\"th\",[t._v(\"符号\")]),t._v(\" \"),n(\"th\",[t._v(\"描述\")])])]),t._v(\" \"),n(\"tbody\",[n(\"tr\",[n(\"td\",[t._v(\".\")]),t._v(\" \"),n(\"td\",[t._v(\"除换行(\\\\n)以外所有字符\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"[ ]\")]),t._v(\" \"),n(\"td\",[t._v(\"[ aoe] [a-w]  匹配集合中任意一个字符\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"\\\\d\")]),t._v(\" \"),n(\"td\",[t._v(\"数字[0-9]\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"\\\\D\")]),t._v(\" \"),n(\"td\",[t._v(\"非数字\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"\\\\w\")]),t._v(\" \"),n(\"td\",[t._v(\"数字，字母，下划线，中文\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"\\\\W\")]),t._v(\" \"),n(\"td\",[t._v(\"非\\\\w\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"\\\\s\")]),t._v(\" \"),n(\"td\",[t._v(\"所有的空白字符\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"\\\\S\")]),t._v(\" \"),n(\"td\",[t._v(\"非空白\")])]),t._v(\" \"),n(\"tr\",[n(\"td\"),t._v(\" \"),n(\"td\")])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"_2-数量修饰\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_2-数量修饰\"}},[t._v(\"#\")]),t._v(\" 2)数量修饰:\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"任意多次  \\t >=0\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"至少1次   \\t >=1\")])]),t._v(\" \"),n(\"table\",[n(\"thead\",[n(\"tr\",[n(\"th\",[t._v(\"符号\")]),t._v(\" \"),n(\"th\",[t._v(\"描述\")])])]),t._v(\" \"),n(\"tbody\",[n(\"tr\",[n(\"td\",[t._v(\"?\")]),t._v(\" \"),n(\"td\",[t._v(\"可有可无  \\t0次  or  1次\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"{m}\")]),t._v(\" \"),n(\"td\",[t._v(\"固定m次\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"{m,}\")]),t._v(\" \"),n(\"td\",[t._v(\"至少m次\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"{m,n}\")]),t._v(\" \"),n(\"td\",[t._v(\"m-n次\")])]),t._v(\" \"),n(\"tr\",[n(\"td\"),t._v(\" \"),n(\"td\")])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"_3-边界\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#_3-边界\"}},[t._v(\"#\")]),t._v(\" 3)边界：\")]),t._v(\" \"),n(\"table\",[n(\"thead\",[n(\"tr\",[n(\"th\",[t._v(\"符号\")]),t._v(\" \"),n(\"th\",[t._v(\"描述\")])])]),t._v(\" \"),n(\"tbody\",[n(\"tr\",[n(\"td\",[t._v(\"\\\\b\")]),t._v(\" \"),n(\"td\")]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"\\\\B\")]),t._v(\" \"),n(\"td\")]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"$\")]),t._v(\" \"),n(\"td\")]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"^\")]),t._v(\" \"),n(\"td\",[t._v(\"以某某开头\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\".*?\")]),t._v(\" \"),n(\"td\",[t._v(\"贪婪模式\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\".+?\")]),t._v(\" \"),n(\"td\",[t._v(\"贪婪模式\")])])])]),t._v(\" \"),n(\"p\",[t._v(\"re.I ： 忽略大小写\\nre.M ：多行匹配\\nre.S ：单行匹配\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"match\")]),t._v(\"\\\\search\\\\findaall\\nre\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"sub\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"正则表达式，替换内容，charset）\\n     图片下载：\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),n(\"br\")])]),n(\"p\",[t._v(\"爬取的需求：（一点点语录网）\")]),t._v(\" \"),n(\"h2\",{attrs:{id:\"bs4库\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#bs4库\"}},[t._v(\"#\")]),t._v(\" bs4库\")]),t._v(\" \"),n(\"p\",[t._v(\"Beautiful Soup\")]),t._v(\" \"),n(\"p\",[t._v(\"需要安装，执行如下命令。\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"pip install bs4 \\npip install lxml\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),n(\"br\")])]),n(\"p\",[n(\"code\",[t._v(\"注意：\")]),t._v(\"需要将pip源改为国类源\")]),t._v(\" \"),n(\"p\",[t._v(\"使用\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"from\")]),t._v(\" bs4 \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" BeautifulSoup （类）\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\")])]),n(\"p\",[t._v(\"使用方法：可以将一个html文档，转化为指定的对象，然后通过对象的方法或者属性去查找指定的内容；\\n1、转化本地文件：\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[t._v(\"soup \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" BeautifulSoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"open\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"本地文件\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"lxml\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\")])]),n(\"p\",[t._v(\"2、将字符串转化为指定对象soup。\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[t._v(\"soup \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" BeautifulSoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"字符串类型或者字节类型\"')]),t._v(\"，‘lxml’\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\")])]),n(\"p\",[t._v(\"​\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"beautifulsoup对象\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#beautifulsoup对象\"}},[t._v(\"#\")]),t._v(\" BeautifulSoup对象\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"根据标签名查找\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#根据标签名查找\"}},[t._v(\"#\")]),t._v(\" 根据标签名查找\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[t._v(\"\\t对象\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"a  \\t\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# 只能找到第一个符合<a>的标签\")]),t._v(\"\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\")])]),n(\"h3\",{attrs:{id:\"获取属性\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#获取属性\"}},[t._v(\"#\")]),t._v(\" 获取属性\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[t._v(\"\\t对象\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"attrs    \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# 获取所有的属性和值，返回一个”字典“\")]),t._v(\"\\n\\t对象\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"attrs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'href'\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\"   \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# 获取href属性\")]),t._v(\"\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),n(\"br\")])]),n(\"h3\",{attrs:{id:\"获取内容\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#获取内容\"}},[t._v(\"#\")]),t._v(\" 获取内容\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[t._v(\"\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"print\")]),t._v(\"（soup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"text）\\n\\tsoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"string\\t\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# 注：如果标签里面还有标签，string不能全部获取内容；\")]),t._v(\"\\n\\tsoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"get_text\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),n(\"br\")])]),n(\"h3\",{attrs:{id:\"find-方法【行为】\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#find-方法【行为】\"}},[t._v(\"#\")]),t._v(\" find()方法【行为】\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[t._v(\"soup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"find\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"     \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# 找到第一个符号要求的a标签的属性和内容\")]),t._v(\"\\nsoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"find\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"title\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"’\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"**\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),t._v(\"‘\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# find通过第二个参数,以标签的属性去-'筛选'-想要找到-'目标'-标签。\")]),t._v(\"\\nsoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"find\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"a\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"class_\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'***'\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"  \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# 注:class属性因为class是关键字,所以是: class_\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#find方法不仅soup可以调用，普通的div对象也能够调用，会\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#去指定的div里面去查找符合要求的 ==》‘节点’\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#find找到的都是‘第一个’符合要求的==》‘标签’\")]),t._v(\"\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),n(\"br\")])]),n(\"h3\",{attrs:{id:\"find-all-return-list\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#find-all-return-list\"}},[t._v(\"#\")]),t._v(\" find_all  [return list]\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-python line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[n(\"code\",[t._v(\"soup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"find_all\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'a'\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\nsoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"find_all\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'a'\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'b'\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\nsoup\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"find_all\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'a'\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"limit\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"2\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"      \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# 限制前两个\")]),t._v(\"\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),n(\"br\"),n(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),n(\"br\")])]),n(\"p\",[t._v(\"单词：limit  -------- /'limit/\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"select-si-lekt\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#select-si-lekt\"}},[t._v(\"#\")]),t._v(\" select/si'lekt/\")]),t._v(\" \"),n(\"p\",[t._v(\"根据选择器选择指定的类容\\n常见的选择器：\\n1.标签选择器\\n2.类选择器\\n3.id选择器\\n4.并集(组合)选择器\\n5.层级选择器\\n6.伪类选择器\\n7.属性选择器\")]),t._v(\" \"),n(\"p\",[t._v(\"上面都是前端css的知识点。\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[t._v(\"select选择器return永远是list，需要通过下标提取指定的对象，然后获取属性和节点\")]),t._v(\" \"),n(\"li\",[t._v(\"该方法也可以通过普通的对象调用，找到都是这个对象下面符合要求的所有的节点\")])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"xpath\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#xpath\"}},[t._v(\"#\")]),t._v(\" xpath\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-shell line-numbers-mode\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-shell\"}},[n(\"code\",[t._v(\"pip \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"install\")]),t._v(\" lxml\\n\")])]),t._v(\" \"),n(\"div\",{staticClass:\"line-numbers-wrapper\"},[n(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),n(\"br\")])]),n(\"div\",{staticClass:\"custom-block tip\"},[n(\"p\",{staticClass:\"custom-block-title\"},[t._v(\"什么是xpath？\")]),t._v(\" \"),n(\"p\",[t._v(\"xml是用来存储和传输数据使用的和html的不同点有两点：\\n(1)html用来显示数据，xml是用来传输数据\\n(2)html标签是固定的，xml标签是自定义的\\n作用：xpath是用来在xml中查找指定的元素，它是一种路径表达式\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"xpath-语法\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#xpath-语法\"}},[t._v(\"#\")]),t._v(\" xpath-语法:\")]),t._v(\" \"),n(\"p\",[t._v(\"常用的路径表达式\")]),t._v(\" \"),n(\"table\",[n(\"thead\",[n(\"tr\",[n(\"th\",[t._v(\"语法\")]),t._v(\" \"),n(\"th\",[t._v(\"描述\")])])]),t._v(\" \"),n(\"tbody\",[n(\"tr\",[n(\"td\",[t._v(\"//\")]),t._v(\" \"),n(\"td\",[t._v(\"不考虑位置的查找\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"./\")]),t._v(\" \"),n(\"td\",[t._v(\"从当前节点开始往下查找\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"../\")]),t._v(\" \"),n(\"td\",[t._v(\"从当前节点的父节点查找（不常用）\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"@\")]),t._v(\" \"),n(\"td\",[t._v(\"选取属性\")])])])]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"实例：\\n路径表达式\\t\\t描述\\n/bookstore/book：  选取根节点bookstore下面所有的book\\n//book：\\t\\t 选取所有book\\nbookstore//book：  查找bookstore下面所有的book\\n/bookstore/book[1]: 查找bookstore里面第一个book\\n/bookstore/book[last()]: bookstore里面的最后一个book\\n/bookstore/book[position()<3]: 选取最前面的两个属于bookstore元素的儿子元素的book元素。\\n//title[@lang]： 选取所有拥有名为lang的属性的title元素（节点）。\\n//title[@lang='eng']: 所有的lang属性值为eng的title节点。\\n\")])])]),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"*：匹配任何元素节点\\n@*： 匹配任何属性节点。\\nnode(): 匹配任何类型的节点。\\n\")])])]),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"安装xpath插件\\n打开xpath快捷键： ctrl + shift+x\\n1.属性定位\\n//input[@id=\\\"kw\\\"]\\n//input[@class=\\\"bg s_btn\\\"]\\n2.层级定位\\n   索引定位\\n\\n//div[@id='head']/div/div[2]/a[@class='toindex']\\n\")])])]),n(\"p\",[t._v('[注意]：索引从1开始。\\n//div[@id=\"head\"]//a[@class=\"toindex\"]\\n[注意]：双斜杠代表下面所有的a节点，不管位置。\\n3.逻辑运算\\nand    逻辑与\\n列：  //input[@class=\"s_ipt\"  and @name=\"wd\"]\\n4.模糊匹配\\ncontains\\n//input[contains(@class, \"s_i\")]\\n所有的input，有class属性，并且属中带s_i的节点。\\nstarts-with\\n//input[starts-with(@class,\"s\")]\\n所有的input，有class属性，并且属性以s开头。\\n6.取文本\\n//div[@id=\"u1\"]/a[5]/text()  获取节点内容\\n//div[@id=\"u1\"]//text()        获取节点里面所有内容，并且不带标签的。\\n7.取属性\\n//div[@id=\"u1\"]/a[5]/@href')]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v('代码怎么操作xpath：\\nfrom lxml import etree\\n1.将html文档变成一个对象，\\n2.然后调用对象的方法去查找指定的node\\n（1）本地文件\\ntree = etree.parse(文件名)\\n（2）网络文件\\ntree = etree.HTML(网页字符串)\\n\\nret = tree.xpath(路径表达式)\\n[注意]：ret返回的是一个列表\\n\\n直接将所有的内容拼接起来\\nret =tree.xpath(\\'//div[@class=\"song\"]\\')\\nstring=ret[0].xpath(\\'string(.)\\')\\nprint(string.replace(\\'\\\\n\\',\" \").replace(\"\\\\t\", \"\"))\\n')])])]),n(\"p\",[t._v(\"[案例]：    http：//www.haoduanzi.com/\\nhttp://www.haoduanzi.com/category/?1-1.html\")]),t._v(\" \"),n(\"p\",[t._v(\"======================================\")]),t._v(\" \"),n(\"p\",[t._v(\"1.图片\\nmeizitu\\nhttp：//sc.chinaz.com/tupian/xingganmeinvtupian.html\")]),t._v(\" \"),n(\"p\",[t._v(\"懒加载技术:\\n用到时候在加载\\n实现方法：\\n\"),n(\"img\",{attrs:{src2:\"图片路经\"}}),t._v(\" \"),n(\"img\",{attrs:{src2:\"\"}})]),t._v(\" \"),n(\"p\",[t._v(\"不懂\")]),t._v(\" \"),n(\"p\",[t._v(\"os.path.exists( )\\nos.path.basename( )\")]),t._v(\" \"),n(\"hr\"),t._v(\" \"),n(\"p\",[t._v(\"2.jsonpath\\njsonpath:  用来解析json数据使用的\\npython处理json格式用到的函数\\njson数据有点像  python 字典 列表\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"import  json\\njson.dumps( )  :将字典  or  列表转化为    json格式的字符串\\n\")])])]),n(\"p\",[t._v(\"￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥\\nimport json\")]),t._v(\" \"),n(\"p\",[t._v(\"lt = [\\n{'name': '王宝强', 'age': 30},\\n{'name': '做杰润', 'age': 36},\\n{'name': '李宇鹏', 'age': 39},\\n{'name': '李玉洁', 'age': 56},\\n]\")]),t._v(\" \"),n(\"p\",[t._v(\"string = json.dumps (lt)\")]),t._v(\" \"),n(\"p\",[t._v(\"obj = json.loads(string)\\nprint(type(obj))  # \"),n(\"class\",{attrs:{list:\"\"}})],1),t._v(\" \"),n(\"p\",[t._v(\"#写             将 lt 写入文件中去\\njson.dump(lt, open('json.txt', 'w', enconding= 'utf-8'))\")]),t._v(\" \"),n(\"p\",[t._v(\"读将文件中的json数据读出来\")]),t._v(\" \"),n(\"p\",[t._v(\"obj = json.load(open('json.txt', 'r', encoding='utf-8'))\\nprint(type(obj))\\nprint(obj)\")]),t._v(\" \"),n(\"p\",[t._v(\"￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥\\njson.loads( )  ：将json格式字符转化为python对象\\njson.dump( ) ：将字典 or 列表转化为  json格式字符串并且写入到文件（file）中\\njson.load( ) ：从文件中读取json格式字符串，转化为python对象\")]),t._v(\" \"),n(\"p\",[t._v(\"记忆： 带s——>string  处理字符串\\n不带s——> 处理 文件\")]),t._v(\" \"),n(\"p\",[t._v(\"前端处理：\\n将json格式的字符串转化为js对象\\nJSON.parse('json格式字符串')\\neval('('+json格式字符串+')')\\t\\n安装：\\npip install  jsonpath\\npip install lxml\")]),t._v(\" \"),n(\"p\",[t._v(\"jsonpath 和 xpath 的对比 -------------------------------\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"xpath\\tjsonpath\\t\\t元素\\n/\\t$\\t\\t根元素\\n.\\t@\\t\\t当前元素\\n/\\t.\\t\\t子元素\\n//\\t..\\t\\t任意位置查找\\n*\\t*\\t\\t通配符\\nxpath 索引下标从0开始\\njsonpath 索引下标从1开始\\n\")])])]),n(\"p\",[t._v(\"https：//blog.csdn.net/luxideyao/article/details/77802389\")]),t._v(\" \"),n(\"p\",[t._v(\"selenium是什么？\\n是以个python的一个第三方库，对象提供的接口可以操作你\\n的浏览器，然后让你的浏览器完成自动化的操作！\\n安装：pip install selenium\\n使用：\\n操作谷歌浏览器，首先必需有谷歌浏览器驱动\")]),t._v(\" \"),n(\"p\",[t._v(\"Selenium [1]  是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera，Edge等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本。\\n代码操作：\\nfrom selenium import webdriver\")]),t._v(\" \"),n(\"p\",[t._v(\"browser = webdriver.Chrome(path)\")]),t._v(\" \"),n(\"p\",[t._v(\"browser.get( )\")]),t._v(\" \"),n(\"p\",[t._v(\"使用下面的方法，查找指定的元素进行操作即可\\nfind_element_by_id\\t\\t\\t根据id找节点\\nfind_element_by_name\\t\\t根据name(属性)找\\nfind_element_by_xpath\\t\\t根据xpath查找\\nfind_element_by_tag_name\\t\\t根据标签名找\\nfind_element_by_class_name\\t\\t根据class名字查找\\nfind_element_by_css_selector\\t\\t根据选择器查找\\nfind_element_by_link_text\\t\\t根据链接内容查找\")]),t._v(\" \"),n(\"p\",[t._v(\"get/send_keys/click\")]),t._v(\" \"),n(\"p\",[t._v(\"+++++++++++++++++++++++++++++++++++++++++\")]),t._v(\" \"),n(\"p\",[t._v(\"PhantomJS\\n是什么？是一款浏览器，是无界面浏览器\\nselenium + PhantomJS 就是爬虫终极解决方案\\n让browser执行简单的js代码，模拟滚动条到底部\")]),t._v(\" \"),n(\"hr\"),t._v(\" \"),n(\"p\",[t._v(\"js ='document.body.scrollTop=10000'\\n#执行js代码\\nbrowser.execute_script(js)\")]),t._v(\" \"),n(\"p\",[t._v(\"#获取网页的代码，保存到文件中\\nhtml = browser.page_source\")]),t._v(\" \"),n(\"p\",[t._v(\"豆瓣电影下拉\\n图片的加载\\n图片加载\\n图片懒加载\\n获取网页的代码： browser.page_source\\n网站：(chinaUnix)\")]),t._v(\" \"),n(\"p\",[t._v(\"day06-爬虫6------------------------------------------------\\n截图\\nbrowser.save_screenshot(image_path)\")]),t._v(\" \"),n(\"p\",[t._v(\"1 Headless Chrome\\n无头谷歌浏览器\\n因为phantomjs现在都不维护了\\nfrom selenium.webdriver.chrome.option import options\\nchrome_options = Option( )\\nchrome_options.add_argument('--headless')\\nchrome_options.add_argument('--disable-gpu')\")]),t._v(\" \"),n(\"p\",[t._v(\"如何使用Headless Chrome\\nHeadless模式是Chrome 59中的新特征。\\n要使用Chrome需要安装-----chromedriver----。\")]),t._v(\" \"),n(\"p\",[t._v(\"《chromedriver》下载地址：http://npm.taobao.org/mirrors/chromedriver/\")]),t._v(\" \"),n(\"p\",[t._v(\"from selenium import webdriver\\nfrom selenium.webdriver.chrome.options import Options\")]),t._v(\" \"),n(\"p\",[t._v(\"chrome_options = Options()\\nchrome_options.add_argument('--headless')\\nchrome_options.add_argument('--disable-gpu')\\ndriver = webdriver.Chrome(chrome_options=chrome_options)\\ndriver.get(\\\"https://cnblogs.com/\\\")\")]),t._v(\" \"),n(\"p\",[t._v(\"2 requests\\n安装 ： pip install requests\\n官方文档：\\nhttp://cn.python-requests.org/zh_CN/latest/\\n用来做什么？\\n和url-lib是同一个位置的\\nget请求\\nurl =‘http://www.baidu.com/’\")]),t._v(\" \"),n(\"p\",[t._v(\"​\\t\\tr = requests.get（url ）\")]),t._v(\" \"),n(\"p\",[t._v(\"响应对象\\nr.text\\t\\t字符串形式查看响应\\nr.content\\t\\t字节类型查看响应\\nr.encoding\\t查看或者设置编码类型\\nr.status_code\\t查看状态码\\nr.headers\\t\\t查看相应头部\\nr.url  \\t\\t查看所请求的url\")]),t._v(\" \"),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"post请求\\n\\n必应翻译\\nformdata：是一个原生字典即可\\nr = requests.post(url=url, headers= headers, data=formdata)\\n\")])])]),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"ajax\\n和上面的一样的\\n\")])])]),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"代理\\nr = requests.get(url, headrs=headers, proxies=proxies)\\n\")])])]),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"cookie\\n实现：人人登入\\n1.创建会话对象\\ns = requests.Session( )\\t能保存post请求cookie的一个对象\\ns.get( )\\ns.post( )\\n登入的思路：\\n1.访问登入页面，获取登入所需要的一些参数\\n2.生成soup对象，获取formhash\\nget \\n2.\\npost\\nget\\n\")])])]),n(\"p\",[t._v(\"单词\\nparameters   参数\")]),t._v(\" \"),n(\"p\",[t._v(\"3.验证码\\ntesserct   app安装\\n检验   tesseract\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"tesseract简介\\n光学识别，但是不要对它期望太高。只能识别简单的代码测试\\n\")])])]),n(\"p\",[t._v(\"pip install pytesseract\\npip install pillow\")]),t._v(\" \"),n(\"p\",[t._v(\"转化为灰色图片\")]),t._v(\" \"),n(\"p\",[t._v(\"img = img.convert('L')\\nimg.show( )\")]),t._v(\" \"),n(\"p\",[t._v(\"二值化处理\")]),t._v(\" \"),n(\"p\",[t._v(\"threshold = 140\\ntable = []\\nfor i in range(256)\\nif i <threshold :\\ntable.append(0)\\nelse:\\ntable.append(1)\\nout = img.point(table,'1')\\nout.show()\")]),t._v(\" \"),n(\"p\",[t._v('img = img.convert(\"RGB\")')]),t._v(\" \"),n(\"p\",[t._v(\"import pytesseract\\nfrom PIL import Image\")]),t._v(\" \"),n(\"p\",[t._v(\"#打开图片\\nimg = Image.open('image_path')\")]),t._v(\" \"),n(\"p\",[t._v(\"img = img.convert('RGB')\")]),t._v(\" \"),n(\"p\",[t._v(\"识别图片\")]),t._v(\" \"),n(\"p\",[t._v('print(pytesseract.image_to_string(\"img\" ))')]),t._v(\" \"),n(\"hr\"),t._v(\" \"),n(\"p\",[t._v(\"day 07--爬虫7\\n=1=，打码平台\\n云打码\\n下载python HTTP接口示列\\n=2=，视频下载\\n哔哔视频为列\\n365yg.com\\n=3=，线程回顾\\n(1).引入\\n多任务，多个任务同时进行，多进程，多线程。\\nword，编辑，检查（多线程）\\nqq，语音，视频，发送消息（多线程）\\n程序中，代码中。\\n(2).创建线程 [Thread]\")]),t._v(\" \"),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"1=面向过程\\n\")])])]),n(\"p\",[t._v(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nthread_obj = (target=xxx, name=xxx, args=xxx,xxx)\\ntarget: 线程启动之后要执行的 functions\\nname ： 线程的name\\n获取线程名字：threading.current_thread().name\\nargs： 主线程向子线程传递参数\\n对象方法：\\n# 启动线程\\nthread_obj.start( )\\n# 让=主线程=等待=子线程=结束\\nthread_obj.start( )\")]),t._v(\" \"),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"2=面向对象&&&\\n\")])])]),n(\"p\",[t._v(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n定义一个类。继承自threading.Thread,重写一个方法（run（））\")]),t._v(\" \"),n(\"p\",[t._v(\"问题：\\n1怎么给线程起名字？\\n2.怎么传递参数？\\n-答-：需要线程的-名字，传递参数，重写构造方法，\\ntip：在重写构造方法的时候，一定要注意手动调用父类的构造方法。\")]),t._v(\" \"),n(\"p\",[t._v(\"子类重写了父类的构造方法，子类要调用父类的构造方法，作用初始化父类的构造方法；\\n如果父类方法有用，子类一定要调用super.\"),n(\"strong\",[t._v(\"init\")]),t._v(\"( )\\nimport threading\")]),t._v(\" \"),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"3=线程同步\\n\")])])]),n(\"p\",[t._v('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n线程之间共享 全局变量，-----很容易发生数据紊乱问题\\n这个时候要用到线程锁\\n理解：同排队上厕所，自己把门一锁。被人想用，等待我解锁\\n坑位如同 -\"全局变量\"-\\n顺序：    抢，谁先抢到。谁上锁之后，谁先用。')]),t._v(\" \"),n(\"p\",[t._v(\"用法\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"# 创建锁\\n\\t锁 = threading.Lock( )\\n# 上锁\\n\\t锁.acquire()\\n\\txxxxxxxxxxxxxxx\\n# 释放锁\\n\\t锁.release( )\\n\")])])]),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"4=队列（queue）\\n\")])])]),n(\"p\",[t._v(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n下载线程\\n解析线程\\n生产者  生产数据\\n消费者  消费数据\\nq = Queue(5)\")]),t._v(\" \"),n(\"p\",[t._v(\"存储数据\")]),t._v(\" \"),n(\"p\",[t._v(\"q.qut('xxx')\\t\\t如果队列满，程序卡在这等待\\nq.qut('xxx', False) \\t\\t如果队列满，程序直接报错\\nq.qut('xxx',Ture， 3) \\t如果队列满，程序等待3s在报错\")]),t._v(\" \"),n(\"p\",[t._v(\"取数据\")]),t._v(\" \"),n(\"p\",[t._v(\"print(q.get( ))\\t\\t如果队列为空，程序卡在这里等待\\nprint(q.get( False))\\t\\t如果队列为空，程序直接报错\\nprint(q.get(Ture， 3 ))\\t如果队列为空，程序等待3s在报错\")]),t._v(\" \"),n(\"p\",[t._v(\"q.empty( )\\t\\t判断队列是否为空\\nq.full( )\\t\\t\\t判断队列是否已经满了\\nq.qsize( )\\t\\t\\t获取队列长度\")]),t._v(\" \"),n(\"p\",[t._v(\"from queue import Queue\")]),t._v(\" \"),n(\"p\",[t._v(\"创建队列\")]),t._v(\" \"),n(\"p\",[t._v(\"q = Queue(5)\")]),t._v(\" \"),n(\"p\",[t._v(\"存储数据\")]),t._v(\" \"),n(\"p\",[t._v(\"q.put('科比')\\nq.put('勒布朗')\\nq.put('JR')\\nq.put('汤普森')\\nq.qut()\\nq.qut()\")]),t._v(\" \"),n(\"p\",[t._v(\"取数据 结果： 先进先出的原则！\")]),t._v(\" \"),n(\"p\",[t._v(\"print(q.get( ))\\nprint(q.get( ))\\nprint(q.get( ))\\nprint(q.get( ))\\nprint(q.get( ))\")]),t._v(\" \"),n(\"p\",[t._v(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\\t\\t\\n#写一个类，继承自threading.Thread\\nclass SingThread(threading.Thread):\\ndef \"),n(\"strong\",[t._v(\"init\")]),t._v(\"(self, name, a):\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"\\tsuper.__init__( )# 调用=父类方法\\n\\tself.name = name\\n\\tself.a = a\\n\\n# 重写了 父类的方法run( )\\ndef run(self):\\n\\tfor x in range(1, 6):\\n\\tprint('我在唱七里香')\\n\\ttime.sleep(1)\\n\\tpass\\n\")])])]),n(\"p\",[t._v(\"class DanceThread(threading.Thread):\\ndef run(self):\\nfor x in range(1, 6):\\nprint('我在跳广场舞')\\ntime.sleep(1)\\npass\\ndef main()\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"tsing = SingThread( ) # 没有构造方法什么也不用填\\t\\ntdance = DanceThread( )\\n\\n#启动线程\\ntsing.start( )\\ntdance.start( )\\n\\n# 让主线程等待子线程结束之后在结束\\ntsing.join( )\\ntdance.join( )\\n\\nprint('主线程和子线程全部结束')\\n\")])])]),n(\"p\",[t._v(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\\n+++++++++++++++++++++++++++++++++++++++++\\nimport time\")]),t._v(\" \"),n(\"p\",[t._v(\"def sing():\\nfor x in range(1, 6):\\nprint('我在唱什么')\\ntime.sleep(1)\")]),t._v(\" \"),n(\"p\",[t._v(\"def dance():\\nfor x in range(1, 6):\\nprint('我在跳hiphop舞')\\ntime.sleep(1)\")]),t._v(\" \"),n(\"p\",[t._v(\"def  main():\\nsing()\\ndance()\")]),t._v(\" \"),n(\"p\",[t._v(\"if \"),n(\"strong\",[t._v(\"name\")]),t._v(' ==\"'),n(\"strong\",[t._v(\"main\")]),t._v('\":\\nmain()\\n小结：  ---先唱---，---在跳---。\\n++++++++++++++++++++++++++++++++++++++++++++\\n一个主线程，两个子线程，(唱歌线程，跳舞线程)\\n多个任务同时进行')]),t._v(\" \"),n(\"p\",[t._v(\"def sing( a):\\nprint('线程为%s接收过来的参数%s' %( threading.current_thread().name, a))\\nfor x in range(1, 6):\\nprint('我在唱什么')\\ntime.sleep(1)\")]),t._v(\" \"),n(\"p\",[t._v(\"def dance(a):\\nprint('线程为%s接收过来的参数%s' %( threading.current_thread().name, a))\\nfor x in range(1, 6):\\nprint('我在跳hiphop舞')\\ntime.sleep(1)\")]),t._v(\" \"),n(\"p\",[t._v(\"def main();\\n# 创建唱歌线程，\\n# name 给线程起一个名字\\na = \\\"孙悟空\\\"\\nt_sing = threading.Thrad(target=sing, name='changge'， args(a,))\\n# 创建跳舞线程，# 通过元组传参\\nt_dance = threading.Thrad(target=dance, name='tiaowu'， args(a,))\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"# 通过对象的方法启动线程\\n#启动线程\\ntsing.start( )\\ntdance.start( )\\n\")])])]),n(\"p\",[t._v(\"让主线程等待子线程结束之后在结束\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"tsing.join( )\\ntdance.join( )\\n\\nprint('这里是主线程')\\n\")])])]),n(\"p\",[t._v(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n单词：\\nthread\\ncurrent\\njoin\\nacquire\\nrelease\")]),t._v(\" \"),n(\"p\",[t._v(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")]),t._v(\" \"),n(\"p\",[t._v(\"4，多线程爬虫\\n分析：\\n两类线程： 下载线程(3个)，解析(3个)\\n内容队列：下载线程 往队列中put数据，解析线程从队列get数据\\nurl队列： =下载线程=从url队列       get 数据\\n写数据：\")]),t._v(\" \"),n(\"p\",[t._v(\"from queue import Queue\\nimport requests\\nform lxml import etree\\nimport json\")]),t._v(\" \"),n(\"p\",[t._v(\"用来存放采集线程\")]),t._v(\" \"),n(\"p\",[t._v(\"g_crawl_list = [ ]\")]),t._v(\" \"),n(\"p\",[t._v(\"用来存放解析线程\")]),t._v(\" \"),n(\"p\",[t._v(\"g_parse_list = [ ]\")]),t._v(\" \"),n(\"p\",[t._v(\"class CrawlThread(threading.Thread):\\ndef \"),n(\"strong\",[t._v(\"init\")]),t._v(\"(self, name, page_queue, data_queue):\\nsuper(CrawlThread, self).\"),n(\"strong\",[t._v(\"init\")]),t._v(\"( )\\nself.name = name\\nself.page_queue = page_queue\\nself.data_queue = data_queue\\nself.url = 'xxxxxxxxxxxxxxxxxx{ }xxx'\\nself.headers = {xxxxxxxxxxxxxxxxxxxxxxx}\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"def run():\\n\\tprint('%s---线程启动'%self.name)\\n\\twhile 1:\\n\\t\\t# 判断采集线程何时退出\\n\\t\\tif self.page_queue.empty():\\n\\t\\t\\tbreak\\n\\t\\t# 从队列中取出页码\\n\\t\\tpage = self.page_queue.get()\\n\\n\\t\\turl =self.url.format(page)\\n\\t\\tr = requests.get(url, headers = self.headers)\\n\\t\\tself.data_queue.put(r.text)\\n\\tprint('%s---线程over' %self.name)\\n\")])])]),n(\"p\",[t._v(\"class ParserThread(threading.Thread):\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"def __init__(self, name, data_queue, fp, lock):\\n\\tsuper(ParserThread, self).__init__( )\\n\\tself.name = name\\n\\tself.page_queue = page_queu\\n\\tself.fp = fp\\n\\tself.lock = lock\\n\\ndef run():\\n\\tprint('%s---线程启动'%self.name)\\n\\twhile 1:\\n\\t\\tdata = self.data_queue.get(True, 10)\\n\\t\\t# 解析内容即可\\n\\t\\tself.parse_content(data)\\n\\n\\tprint('%s---线程over' %self.name)\\n\\ndef parse_content(self， data)：\\n\\ttree = etree.HTML(data)\\n\\t\\n\\tli_list = tree.xpath('//ul[@class=\\\"cont-list\\\"]/li')\\n\\t\\n\\titems =[ ]\\n\\n\\tfor oli in li_list:\\n\\t\\ttitle = oli.xpath('//h2/a/text()')[0]\\n\\t\\timage_url = oli.xpath('.//div[contains(@class, \\\"cont-list-maun\\\")]//img/@data-src')\\n\\t\\titem = {\\n\\n\\t\\t\\\"标题\\\"：title，\\n\\t\\t\\\"链接\\\": image_url\\n\\t\\t}\\n\\t\\t\\n\\t\\titems.append(item)\\n\\t\\n\\t# 上锁\\n\\tself.lock.acquire()\\n\\tself.fp.write(json.dumps(items, ensurs_ascii=False+'\\\\n'))\\n\\t# 释放锁\\n\\tself.lock.release( )\\n\")])])]),n(\"p\",[t._v(\"def create_queue():\\n# 创建页码队列\\npage_queue = Queue()\\nfor page in range(1, 11):\\npage_queue.put(page)\\n# 创建内容队列\\ndata_queue = Queue()\\nreturn page_queue, data_queue\")]),t._v(\" \"),n(\"p\",[t._v(\"创建采集线程\")]),t._v(\" \"),n(\"p\",[t._v(\"def create_crawl_thread(page_queue, data_queue):\\ncrawl_name = ['采集线程1号', '采集线程2号','采集线程3号']\\nfor name in crawl_name:\\n# 创建一个采集线程\\ntcrawl = CrawlThread(name, page_queue, data_queue)\\n# 保存到列表中\\ng_crawl_list.append(tcrawl)\")]),t._v(\" \"),n(\"p\",[t._v(\"创建解析线程\")]),t._v(\" \"),n(\"p\",[t._v(\"def create_parse_thread(page_queue, data_queue):\\nparse_name = ['解析线程1号', '解析线程2号','解析线程3号']\\nfor name in parse_name:\\n# 创建一个解析线程\\ntparse = CrawlThread(name, data_queue, fp, lock)\\n# 保存到列表中\\ng_parse_list.append(tparse)\")]),t._v(\" \"),n(\"p\",[t._v(\"def main():\\n# 创建队列函数\\npage_queue, data_queue = create_queue()\\n# 打开文件\")]),t._v(\" \"),n(\"p\",[t._v(\"线程之间不共享局部变量\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"fp = open('jian.json', 'a', encoding = 'utf-8')\\n# 创建锁\\nlock = treading.Lock()\\n# 创建采集线程\\ncreate_crawl_thread(page_queue, data_queue)\\ntime.sleep(3)\\n# 创建解析线程\\ncreate_parse_thread(data_queue, fp, lock)\\n\\n# 启动所有采集线程\\nfor tcrawl in g_crawl_list:\\n\\ttcrawl.start()\\n# 启动所有解析线程\\nfor tparse in g_parse_list:\\n\\ttparse.start()\\n\")])])]),n(\"p\",[t._v(\"让主线程等待子线程结束之后在结束\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"for tcrawl in g_crawl_list:\\n\\ttcrawl.join()\\nfor tparse in g_parse_list:\\n\\ttparse.join()\\n\\nfp.close()\\n\")])])]),n(\"p\",[t._v(\"=============================================\")]),t._v(\" \"),n(\"p\",[t._v(\"？？？？？？？？？？？？？？？？？？？？？？？？？？？\\nday08---爬虫8\")]),t._v(\" \"),n(\"p\",[t._v(\"1.scrapy初认识\\nscrapy是什么？\\n是python的一个爬虫的框架，非常出名，非常强悍，\\n学的就是用法，当然，底层肯定使用了多进程，多线程，队列等及技术。\")]),t._v(\" \"),n(\"p\",[t._v(\"安装 ： pip install  scrapy\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"框架有5部分组成\\n引擎，下载器，spiders，调度器(schedule),管道(pipeline)\\n我们的代码写到spiders， 管道中，\\n\")])])]),n(\"p\",[t._v(\"spiders： 文件内容解析，链接提取\\npipeline ： 数据是保存到文件中，mysql中，MongoDB中？\")]),t._v(\" \"),n(\"p\",[t._v(\"工作原理：见图\\n简单使用：\\n(1)创建项目\\nscrapy  startproject [项目名]firstblood\\n(2)认识目录结构\\nfirstblood\\nfirstblood   \\t正真的项目文件\\n\"),n(\"strong\",[t._v(\"pycache\")]),t._v(\"\\nspiders\\t\\t爬虫文件存放的地方\\n\"),n(\"strong\",[t._v(\"pycache\")]),t._v(\" \"),n(\"strong\",[t._v(\"init\")]),t._v(\".py\\nnew_file.py\\t爬虫文件 (\"),n(\"em\",[t._v(\")\\t\\n\"),n(\"strong\",[t._v(\"init\")]),t._v(\".py\\t代表firstblood是包\\nitems.py\\t\\t定义数据结构的地方 (\")]),t._v(\")\\nmiddle-wares.py\\t中间件\\npipelines.py\\t管道文件\\t(\"),n(\"em\",[t._v(\")\\nsettings.py\\t配置文件(\")]),t._v(\")\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"\\tscrapy.cfg\\t不用管\\n\\n(3)生成爬虫文件\\n\")])])]),n(\"p\",[t._v(\"·\\t第一：\\ncd [项目名]\\ncd firstblood\\n第二:\\nscrapy genspider qiubai[名字]  域名\\n---------------------------------------------代码\\nclass QiubaiSpider(scrapy.Spider):\\n# 爬虫的名字\\nname = 'quibai'\\n# 允许的 -域名, 是一个列表，里面可以放多个，一般都做限制\\nallowed_domains = ['www.qiushibaike.com']\\n# 起始url，是一个列表\\nstart_urls = ['http://www.quishibaike.com']\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"\\t# 解析解析函数， 重写这个方法，\\n\")])])]),n(\"p\",[t._v(\"发送请求之后，响应来了就会调用这个方法，函数就有一个参数response就是响应内容，该\\n函数对返回值有一个要求，必须返回--可迭代对象\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"\\tdef parse(self, response):\\n\\t\\tpass\\n\")])])]),n(\"hr\"),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"(4)认识response对象\\n程序跑起来\\ncd firstblood/firstblood/spider\\nscrapy crawl qiubai[用的是QiubaiSpider类中的属性name]\\n\\n(问题1)pywin32安装一下，\\n(问题2)取消遵从robots协议\\n\\tsettings.py中第22行\\n(问题3)修改UA头部信息\\n\\tsettings.py中第19行\\nresponse的常用 =方法=  和  =属性=\\n\\ttext:\\t字符串类型\\n\\tbody:\\t字节类型\\n\")])])]),n(\"p\",[t._v(\"=方法=================\\nxpath():\\t\\tscrapy内部已经集成了xpath,直接使用即可，\\n此xpath非彼xpath，略有不同。\\nextract( )\\n(5)执行输出指定格式\\nscrapy crawl qiubai -o qiubai.json\\nscrapy crawl qiubai -o qiubai.xml\\nscrapy crawl qiubai -o qiubai.csv\\n【注】：你输出为csv的时候，中间估计有空行，自己百度一下解决掉。\\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")]),t._v(\" \"),n(\"p\",[t._v(\"2.scrapy shell\\nscrapy shell是什么？\\n调试工具，常用来调试xpath对不对\\n安装依赖：pip install ipython\\n作用：\\t更加智能的交互环境\\n可以通过tab键智能提醒\\n终端下任意位置，输入如下指令：\\nscrapy sheel url\\n进行交互\\n++++++++++++++++++++++++++\\nresponse\\n-----------\\t属性\\ntext\\t字符串类型\\nbody\\t字节类型\\nurl\\t请求的url\\nstatus\\t响应状态码\\nheaders\\t响应头\\n-----------\\t方法\\nxpath( )\\t提取出来的多是selector对象，需要进行extract( )\\ncss( ) \\t根据选择器进行获取指定的内容\\n# content-left >div>div>a>img\\nret = response.css('#content-left>div>.author img::attr(src)')#  提取img 中的src的\\\\内容\\n【注意】这种获取属性的方式只能在scrapy中使用，bs中不能这么使用\\nret = response.css('#content-left>div>.author h1::text')# 提取h1 中的文本内容\\n【注】这中方式获取的列表，也得extract一下，才能得到你想要的字符串。\\nselector 对象\\n是scrapy自己封装的一个对象，不论你是通过xpath还是css，获取到的都是\\n这个对象，\\n=+++++对象方法++++++==\\nxpath( )\\ncss( )\\nextract( )\\t\\t将对象直接转化为字符串\\nextract_first( )\\t功能就等同于\\nextract_first() == [0].extract() == extract()[0]\\n如果xpath或者css写错了，返回的是空列表。那么通过后两种方式获取的时候就会报错\\n但是通过extract_first()获取，会返会None\\nitem对象\\n爬取数据的时候，第一步要定义数据结构，在items.py种定义，\\n通过这个类创建的对象非常特殊，这个=对象=使用的时候类似字典\\n而且这个对象可以方便的转化为字典\\nclass Person(scrapy.Item):\\nname = scrapy.Field()\\nage = scrapy.Field()\\np = Person()\\n赋值和使用\\np['name'] = 'goudan'\\np['age'] = 20\\np[name]  p[\\\"age\\\"]\\n转化为字典\\nd = dict()\")]),t._v(\" \"),n(\"p\",[t._v(\"3 yield item 和 请求\\nyield是什么意思？\\n答: 函数中出现yield，代表这个函数是一个生成器，【注意】：函数可以出现\\n多个yield\\n通过scrapy 来爬稠视百科\\nDOWNLOAD_DELAY = 3  下载延时\\nITEM_PIPELINES = {\\n'xxxxx.xxxxxxxxxx.xxxxxPipeline':300,\\n}\")]),t._v(\" \"),n(\"p\",[t._v(\"列子：\\ndef test():\\nlt = [ ]\\nfor x in range(1, 11):\\nlt.append(x)\\nreturn lt\")]),t._v(\" \"),n(\"p\",[t._v(\"生成器【generator】，不是保存的数据，而是保存的这个数据生成的方式，用到的时候，直接调用，再给你生成\")]),t._v(\" \"),n(\"p\",[t._v('def demo()：\\nfor x in range(1, 11):\\nyield x\\nprint(\"嘿嘿嘿\")\\n# 请问打印一个1的时候会打印-嘿嘿嘿-吗？？？\\n答： ON\\n因为 ：生成器的作用：就是 【口诀】《 用一个 ，就拿一，》\\n不用就不生成，用就生成。节约了内存空间。\\na = demo()\\nprint(a) # 是个generator     可以遍历a')]),t._v(\" \"),n(\"p\",[t._v(\"取数据\")]),t._v(\" \"),n(\"p\",[t._v(\"print(next(a))  # 1\\nprint(next(a))  # 2\\nprint(next(a))  # 3\")]),t._v(\" \"),n(\"p\",[t._v(\"？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？\")]),t._v(\" \"),n(\"p\",[t._v(\"day09-爬虫9\")]),t._v(\" \"),n(\"p\",[t._v(\"网页\\nwww.521609.com\")]),t._v(\" \"),n(\"p\",[t._v(\"电影网id97\")]),t._v(\" \"),n(\"p\",[t._v(\"1，下载图片\")]),t._v(\" \"),n(\"p\",[t._v(\"2，请求传参\\n提取的数据不在一个页面\")]),t._v(\" \"),n(\"p\",[t._v(\"3，crawlspider\\nLinkExtractor(\\nallow=xxx,\\t#正则表达式，要(\"),n(\"em\",[t._v(\")\\ndeny=xxx,\\t#正则表达式，不要这个\\nrestrict_xpaths=xxx,\\t# xpath路径（\")]),t._v(\"）\\nrestrict_css=xxx,\\t\\t# 选择器（*）\\ndeny_domains=xxx,\\t# 不允许的域名\")]),t._v(\" \"),n(\"p\",[t._v(\")\")]),t._v(\" \"),n(\"p\",[t._v(\"LinkEXtractor是一个类\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",[n(\"code\",[t._v(\"用法演示：\\n\\tscrapy  shell  http://www.id97.com/movie\\n\\tfrom scrapy.linkextractors import LnkExtractor\\n\")])])]),n(\"p\",[t._v(\"第一种提取方式：\\t\\n======\\t通过正则表达提取链接\\nlinks = LinkExtractor(allow=r'/movie/?page=\\\\d')\\n将所有包含这个正则表达式的href全部获取到，并返回\\nlinks.tract_links(response)进行查看提取到的链接\\n【注意】将重复的url去除掉\")]),t._v(\" \"),n(\"p\",[t._v(\"第二种提取方式：\\t\\n======\\t通过xpath提取\\nlinks = LinkExtractor(restrict_xpaths=xxx,)\\nlinks = LinkExtractor(restrict_xpaths=' //ul[@class=\\\"pagination pagination-sm\\\"]/li/a ')\")]),t._v(\" \"),n(\"p\",[t._v(\"第三种提取方式：\\t\\n======通过  【css选择器 +  > + 标签名字】   提取\\nlinks = LinkExtractor(restrict_css=xxx,)\\n使用：\\nfollow ： 是否 跟进\\n什么是  \\t是否 跟进？\\n第一页提取的页码是： 2 3 4 5 6\")]),t._v(\" \"),n(\"p\",[t._v(\"4,日志信息和等级\\nCRITICAL : 严重错误\\nERROR：　一般错误\\nWARNING：　警告\\nINFO：　一般的信息\\nDEBUG：　调试信息\\n默认的是显示级别是DEBUG\\n========配置代码==============\\nＬＯＧ＿ＬＥＶＥＬ　＝　＇ＤＥＢＵＧ＇\\t级别是debug\\nＬＯＧ＿ＦＩＬＥ　＝　'log.txt'\\t\\t将调试信息显示到指定的文件中。　　　　\\t\\n5，发送post请求\\n直接发送post请求，需要将start_urls注释掉，然后重写start_requests方法，在这个方法\\n里面直接发送post请求\\nyield scrapy.FormRequest(url= post_url, formdata=formdata, callback.parse)\")]),t._v(\" \"),n(\"p\",[t._v(\"？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？\")]),t._v(\" \"),n(\"p\",[t._v(\"day10-爬虫10\")]),t._v(\" \"),n(\"p\",[t._v(\"1.代理的使用\\n下载中间件\\n2.模拟登入\\n豆瓣（未看）\")]),t._v(\" \"),n(\"p\",[t._v(\"3.存储到MySQL，mongodb\")]),t._v(\" \"),n(\"p\",[t._v(\"​\")])])}),[],!1,null,null,null);n.default=e.exports}}]);","extractedComments":[]}